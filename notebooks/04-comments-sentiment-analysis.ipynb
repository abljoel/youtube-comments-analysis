{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper libraries\n",
    "import warnings\n",
    "\n",
    "# Scientific and visual libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Local Modules\n",
    "from youtube_analysis.paths import TRANSFORMED_DATA_DIR\n",
    "from youtube_analysis.cleanutils import translate_emojis\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Various settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_colwidth\", 40)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an unsupervised method for that because labeling all comments will be time-consuming and challeging. For that, we can use a lexicon and rule-based method: VADER. It is included in the NLTK library. The result may not be as accurate as expected since the valence dictionary is not context-specific in VADER, but it comes to be enough for most social media content.\n",
    "\n",
    "Let's load our preprocessed corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_pickle(TRANSFORMED_DATA_DIR / \"processed_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Sentiment Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create two functions: one for categorizing, and one for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def get_sent_label(text=\"\", score=None):\n",
    "    if not score:\n",
    "        score = sent_analyzer.polarity_scores(text)[\"compound\"]\n",
    "    if 0.4 < score:\n",
    "        return \"positive\"\n",
    "    if -0.1 < score <= 0.4:\n",
    "        return \"neutral\"\n",
    "    return \"negative\"\n",
    "\n",
    "\n",
    "def get_sent_score(text):\n",
    "    score = sent_analyzer.polarity_scores(text)[\"compound\"]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the functions to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"sent_class\"] = corpus.cleaned_text.apply(lambda t: get_sent_label(t))\n",
    "corpus[\"sent_score\"] = corpus.cleaned_text.apply(lambda t: get_sent_score(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>sent_class</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>here are the timestamps   please che...</td>\n",
       "      <td>timestamps please check sponsors sup...</td>\n",
       "      <td>timestamps please check sponsor supp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>protein research is a major new brea...</td>\n",
       "      <td>protein research major new breakthrough</td>\n",
       "      <td>protein research major new breakthrough</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>this is a good one</td>\n",
       "      <td>good one</td>\n",
       "      <td>good one</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>a very interesting conversation unti...</td>\n",
       "      <td>interesting conversation end answer ...</td>\n",
       "      <td>interesting conversation end answer ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.8960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>this chick is always in bilderberg g...</td>\n",
       "      <td>chick always bilderberg group meetin...</td>\n",
       "      <td>chick always bilderberg group meetin...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>now i feel really infinitely ignoran...</td>\n",
       "      <td>feel really infinitely ignorant hope...</td>\n",
       "      <td>feel really infinitely ignorant hope...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.3976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>i kept checking the time in hopes th...</td>\n",
       "      <td>kept checking time hopes entered sta...</td>\n",
       "      <td>kept checking time hope entered stat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steve C</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>😳The Plants are Fake…?</td>\n",
       "      <td>the plants are fake</td>\n",
       "      <td>plants fake</td>\n",
       "      <td>plant fake</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brigid Mary Prain</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "      <td>responding to your pushback on the p...</td>\n",
       "      <td>responding pushback pushback circa i...</td>\n",
       "      <td>responding pushback pushback circa i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Reacher</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://www.youtube.com/wat...</td>\n",
       "      <td>happy face smiley         brutal...</td>\n",
       "      <td>happy face smiley brutal facts plane...</td>\n",
       "      <td>happy face smiley brutal fact planet...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author              published_at                updated_at  \\\n",
       "0          Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1       John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2       John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3     john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4      arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5  WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6            Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "7              Steve C 2023-11-10 22:16:58+00:00 2023-11-10 22:16:58+00:00   \n",
       "8    Brigid Mary Prain 2023-11-09 09:51:33+00:00 2023-11-09 09:51:33+00:00   \n",
       "9         Jack Reacher 2023-11-04 09:30:38+00:00 2023-11-04 09:30:38+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "5      0  Now I feel really infinitely ignoran...   \n",
       "6      0  I kept checking the time in hopes th...   \n",
       "7      0                   😳The Plants are Fake…?   \n",
       "8      0  Responding to your pushback on the p...   \n",
       "9      0  <a href=\"https://www.youtube.com/wat...   \n",
       "\n",
       "                              cleaned_text  \\\n",
       "0  here are the timestamps   please che...   \n",
       "1  protein research is a major new brea...   \n",
       "2                     this is a good one     \n",
       "3  a very interesting conversation unti...   \n",
       "4  this chick is always in bilderberg g...   \n",
       "5  now i feel really infinitely ignoran...   \n",
       "6  i kept checking the time in hopes th...   \n",
       "7                   the plants are fake      \n",
       "8  responding to your pushback on the p...   \n",
       "9      happy face smiley         brutal...   \n",
       "\n",
       "                             filtered_text  \\\n",
       "0  timestamps please check sponsors sup...   \n",
       "1  protein research major new breakthrough   \n",
       "2                                 good one   \n",
       "3  interesting conversation end answer ...   \n",
       "4  chick always bilderberg group meetin...   \n",
       "5  feel really infinitely ignorant hope...   \n",
       "6  kept checking time hopes entered sta...   \n",
       "7                              plants fake   \n",
       "8  responding pushback pushback circa i...   \n",
       "9  happy face smiley brutal facts plane...   \n",
       "\n",
       "                           lemmatized_text sent_class  sent_score  \n",
       "0  timestamps please check sponsor supp...   positive      0.9922  \n",
       "1  protein research major new breakthrough    neutral      0.0000  \n",
       "2                                 good one   positive      0.4404  \n",
       "3  interesting conversation end answer ...   negative     -0.8960  \n",
       "4  chick always bilderberg group meetin...    neutral      0.0000  \n",
       "5  feel really infinitely ignorant hope...   negative     -0.3976  \n",
       "6  kept checking time hope entered stat...   negative     -0.4767  \n",
       "7                               plant fake   negative     -0.4767  \n",
       "8  responding pushback pushback circa i...   positive      0.5859  \n",
       "9  happy face smiley brutal fact planet...   positive      0.6705  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to what has been said sometimes about VADER, it can't classify directly emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sent_label(\"😍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sent_label(translate_emojis(\"😍\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this new dataset into a different file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_pickle(TRANSFORMED_DATA_DIR / \"sentiment_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
