{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper libraries\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Scientific and visual libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from emot import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Various settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_colwidth\", 40)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import useful paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_analysis.paths import INTERIM_DATA_DIR, TRANSFORMED_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reload the interim dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_pickle(INTERIM_DATA_DIR / \"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text falls into the definition of unstructured data. It means that while it is easily interpretable by humans, it's not the case for computers. This difference leads to very different strategies for processing and analysis, and errors can take advantage of human intuition. In order to use text corpus as information-rich features, we must process it in the right way.\n",
    "\n",
    "For that we will define special functions dedicated to basic text normalization. Let's dig into the steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs and HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>This is a good one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author              published_at                updated_at  \\\n",
       "0       Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1    John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2    John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3  john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4   arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "\n",
       "                              cleaned_text  \n",
       "0  Here are the timestamps. Please chec...  \n",
       "1  Protein research is a major new brea...  \n",
       "2                      This is a good one.  \n",
       "3  A very interesting conversation unti...  \n",
       "4  This chick is always in Bilderberg g...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"cleaned_text\"] = corpus.text.apply(lambda t: remove_html_tags(t))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Emojis and Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emojis(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    for emo in UNICODE_EMOJI:\n",
    "        if emo in tokenized_text:\n",
    "            emo_index = tokenized_text.index(emo)\n",
    "            tokenized_text[emo_index] = UNICODE_EMOJI[emo].replace(\":\",\"\").replace(\"_\", \" \")\n",
    "    return \" \".join(tokenized_text)\n",
    "\n",
    "\n",
    "def translate_emoticons(text):\n",
    "    new_text = text\n",
    "    for ticon in EMOTICONS_EMO:\n",
    "        if ticon in new_text:\n",
    "            new_text = new_text.replace(ticon, EMOTICONS_EMO[ticon])\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"cleaned_text\"] = corpus.cleaned_text.apply(lambda t: translate_emojis(t))\n",
    "corpus[\"cleaned_text\"] = corpus.cleaned_text.apply(lambda t: translate_emoticons(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>Here are the timestamps . Please che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>This is a good one .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steve C</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>😳The Plants are Fake…?</td>\n",
       "      <td>😳The Plants are Fake… ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brigid Mary Prain</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Reacher</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://www.youtube.com/wat...</td>\n",
       "      <td>1:44Happy face smiley5 `` ... brutal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author              published_at                updated_at  \\\n",
       "0          Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1       John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2       John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3     john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4      arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5  WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6            Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "7              Steve C 2023-11-10 22:16:58+00:00 2023-11-10 22:16:58+00:00   \n",
       "8    Brigid Mary Prain 2023-11-09 09:51:33+00:00 2023-11-09 09:51:33+00:00   \n",
       "9         Jack Reacher 2023-11-04 09:30:38+00:00 2023-11-04 09:30:38+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "5      0  Now I feel really infinitely ignoran...   \n",
       "6      0  I kept checking the time in hopes th...   \n",
       "7      0                   😳The Plants are Fake…?   \n",
       "8      0  Responding to your pushback on the p...   \n",
       "9      0  <a href=\"https://www.youtube.com/wat...   \n",
       "\n",
       "                              cleaned_text  \n",
       "0  Here are the timestamps . Please che...  \n",
       "1  Protein research is a major new brea...  \n",
       "2                     This is a good one .  \n",
       "3  A very interesting conversation unti...  \n",
       "4  This chick is always in Bilderberg g...  \n",
       "5  Now I feel really infinitely ignoran...  \n",
       "6  I kept checking the time in hopes th...  \n",
       "7                  😳The Plants are Fake… ?  \n",
       "8  Responding to your pushback on the p...  \n",
       "9  1:44Happy face smiley5 `` ... brutal...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all emojis have been translated, thus we need to finalize the task by removing non alphabetic symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non Alphabetic Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_noise(text):\n",
    "    next_text = text\n",
    "    for e in next_text:\n",
    "        if not e.isalpha():\n",
    "            next_text = next_text.replace(e, \" \")\n",
    "    return next_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>Here are the timestamps   Please che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>This is a good one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steve C</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>😳The Plants are Fake…?</td>\n",
       "      <td>The Plants are Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brigid Mary Prain</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Reacher</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://www.youtube.com/wat...</td>\n",
       "      <td>Happy face smiley         brutal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author              published_at                updated_at  \\\n",
       "0          Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1       John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2       John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3     john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4      arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5  WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6            Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "7              Steve C 2023-11-10 22:16:58+00:00 2023-11-10 22:16:58+00:00   \n",
       "8    Brigid Mary Prain 2023-11-09 09:51:33+00:00 2023-11-09 09:51:33+00:00   \n",
       "9         Jack Reacher 2023-11-04 09:30:38+00:00 2023-11-04 09:30:38+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "5      0  Now I feel really infinitely ignoran...   \n",
       "6      0  I kept checking the time in hopes th...   \n",
       "7      0                   😳The Plants are Fake…?   \n",
       "8      0  Responding to your pushback on the p...   \n",
       "9      0  <a href=\"https://www.youtube.com/wat...   \n",
       "\n",
       "                              cleaned_text  \n",
       "0  Here are the timestamps   Please che...  \n",
       "1  Protein research is a major new brea...  \n",
       "2                     This is a good one    \n",
       "3  A very interesting conversation unti...  \n",
       "4  This chick is always in Bilderberg g...  \n",
       "5  Now I feel really infinitely ignoran...  \n",
       "6  I kept checking the time in hopes th...  \n",
       "7                   The Plants are Fake     \n",
       "8  Responding to your pushback on the p...  \n",
       "9      Happy face smiley         brutal...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"cleaned_text\"] = corpus.cleaned_text.apply(lambda t: filter_text_noise(t))\n",
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"cleaned_text\"] = corpus.cleaned_text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>here are the timestamps   please che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>protein research is a major new brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>this is a good one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>a very interesting conversation unti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>this chick is always in bilderberg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>now i feel really infinitely ignoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>i kept checking the time in hopes th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steve C</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>😳The Plants are Fake…?</td>\n",
       "      <td>the plants are fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brigid Mary Prain</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "      <td>responding to your pushback on the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Reacher</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://www.youtube.com/wat...</td>\n",
       "      <td>happy face smiley         brutal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Oskar Gudnason</td>\n",
       "      <td>2023-11-02 21:47:54+00:00</td>\n",
       "      <td>2023-11-03 12:18:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Don´t you forget that the ape comes ...</td>\n",
       "      <td>don t you forget that the ape comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Boggo</td>\n",
       "      <td>2023-11-01 09:06:38+00:00</td>\n",
       "      <td>2023-11-01 09:06:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Does this host not bother to speak c...</td>\n",
       "      <td>does this host not bother to speak c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Imperious</td>\n",
       "      <td>2023-10-31 20:03:41+00:00</td>\n",
       "      <td>2023-10-31 20:03:41+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is dumb, the only answer is God...</td>\n",
       "      <td>this is dumb   the only answer is go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>2023-10-30 14:01:05+00:00</td>\n",
       "      <td>2023-10-30 14:01:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Annnd they ...... 😅 Awesomeness!</td>\n",
       "      <td>annnd they        grinning face with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indie Guvenc</td>\n",
       "      <td>2023-10-30 09:39:49+00:00</td>\n",
       "      <td>2023-10-30 09:46:12+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>@&lt;a href=\"https://www.youtube.com/wa...</td>\n",
       "      <td>lex is completely dominated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author              published_at                updated_at  \\\n",
       "0           Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1        John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2        John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3      john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4       arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5   WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6             Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "7               Steve C 2023-11-10 22:16:58+00:00 2023-11-10 22:16:58+00:00   \n",
       "8     Brigid Mary Prain 2023-11-09 09:51:33+00:00 2023-11-09 09:51:33+00:00   \n",
       "9          Jack Reacher 2023-11-04 09:30:38+00:00 2023-11-04 09:30:38+00:00   \n",
       "10       Oskar Gudnason 2023-11-02 21:47:54+00:00 2023-11-03 12:18:47+00:00   \n",
       "11                Boggo 2023-11-01 09:06:38+00:00 2023-11-01 09:06:38+00:00   \n",
       "12            Imperious 2023-10-31 20:03:41+00:00 2023-10-31 20:03:41+00:00   \n",
       "13            Elizabeth 2023-10-30 14:01:05+00:00 2023-10-30 14:01:05+00:00   \n",
       "14         Indie Guvenc 2023-10-30 09:39:49+00:00 2023-10-30 09:46:12+00:00   \n",
       "\n",
       "    likes                                     text  \\\n",
       "0     194  Here are the timestamps. Please chec...   \n",
       "1       0  Protein research is a major new brea...   \n",
       "2       0                      This is a good one.   \n",
       "3       0  A very interesting conversation unti...   \n",
       "4       0  This chick is always in Bilderberg g...   \n",
       "5       0  Now I feel really infinitely ignoran...   \n",
       "6       0  I kept checking the time in hopes th...   \n",
       "7       0                   😳The Plants are Fake…?   \n",
       "8       0  Responding to your pushback on the p...   \n",
       "9       0  <a href=\"https://www.youtube.com/wat...   \n",
       "10      0  Don´t you forget that the ape comes ...   \n",
       "11      0  Does this host not bother to speak c...   \n",
       "12      0  This is dumb, the only answer is God...   \n",
       "13      0         Annnd they ...... 😅 Awesomeness!   \n",
       "14      0  @<a href=\"https://www.youtube.com/wa...   \n",
       "\n",
       "                               cleaned_text  \n",
       "0   here are the timestamps   please che...  \n",
       "1   protein research is a major new brea...  \n",
       "2                      this is a good one    \n",
       "3   a very interesting conversation unti...  \n",
       "4   this chick is always in bilderberg g...  \n",
       "5   now i feel really infinitely ignoran...  \n",
       "6   i kept checking the time in hopes th...  \n",
       "7                    the plants are fake     \n",
       "8   responding to your pushback on the p...  \n",
       "9       happy face smiley         brutal...  \n",
       "10  don t you forget that the ape comes ...  \n",
       "11  does this host not bother to speak c...  \n",
       "12  this is dumb   the only answer is go...  \n",
       "13  annnd they        grinning face with...  \n",
       "14          lex is completely dominated ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = []\n",
    "    for token in tokenized_text:\n",
    "        if token not in stop_words:\n",
    "            filtered_text.append(token)\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>here are the timestamps   please che...</td>\n",
       "      <td>timestamps please check sponsors sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>protein research is a major new brea...</td>\n",
       "      <td>protein research major new breakthrough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>this is a good one</td>\n",
       "      <td>good one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>a very interesting conversation unti...</td>\n",
       "      <td>interesting conversation end answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>this chick is always in bilderberg g...</td>\n",
       "      <td>chick always bilderberg group meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>now i feel really infinitely ignoran...</td>\n",
       "      <td>feel really infinitely ignorant hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>i kept checking the time in hopes th...</td>\n",
       "      <td>kept checking time hopes entered sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steve C</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>2023-11-10 22:16:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>😳The Plants are Fake…?</td>\n",
       "      <td>the plants are fake</td>\n",
       "      <td>plants fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brigid Mary Prain</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>2023-11-09 09:51:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Responding to your pushback on the p...</td>\n",
       "      <td>responding to your pushback on the p...</td>\n",
       "      <td>responding pushback pushback circa i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Reacher</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>2023-11-04 09:30:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://www.youtube.com/wat...</td>\n",
       "      <td>happy face smiley         brutal...</td>\n",
       "      <td>happy face smiley brutal facts plane...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author              published_at                updated_at  \\\n",
       "0          Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1       John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2       John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3     john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4      arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5  WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6            Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "7              Steve C 2023-11-10 22:16:58+00:00 2023-11-10 22:16:58+00:00   \n",
       "8    Brigid Mary Prain 2023-11-09 09:51:33+00:00 2023-11-09 09:51:33+00:00   \n",
       "9         Jack Reacher 2023-11-04 09:30:38+00:00 2023-11-04 09:30:38+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "5      0  Now I feel really infinitely ignoran...   \n",
       "6      0  I kept checking the time in hopes th...   \n",
       "7      0                   😳The Plants are Fake…?   \n",
       "8      0  Responding to your pushback on the p...   \n",
       "9      0  <a href=\"https://www.youtube.com/wat...   \n",
       "\n",
       "                              cleaned_text  \\\n",
       "0  here are the timestamps   please che...   \n",
       "1  protein research is a major new brea...   \n",
       "2                     this is a good one     \n",
       "3  a very interesting conversation unti...   \n",
       "4  this chick is always in bilderberg g...   \n",
       "5  now i feel really infinitely ignoran...   \n",
       "6  i kept checking the time in hopes th...   \n",
       "7                   the plants are fake      \n",
       "8  responding to your pushback on the p...   \n",
       "9      happy face smiley         brutal...   \n",
       "\n",
       "                             filtered_text  \n",
       "0  timestamps please check sponsors sup...  \n",
       "1  protein research major new breakthrough  \n",
       "2                                 good one  \n",
       "3  interesting conversation end answer ...  \n",
       "4  chick always bilderberg group meetin...  \n",
       "5  feel really infinitely ignorant hope...  \n",
       "6  kept checking time hopes entered sta...  \n",
       "7                              plants fake  \n",
       "8  responding pushback pushback circa i...  \n",
       "9  happy face smiley brutal facts plane...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"filtered_text\"] = corpus.cleaned_text.apply(lambda t: filter_stopwords(t))\n",
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in tokenized_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lex Fridman</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>2022-12-29 17:34:04+00:00</td>\n",
       "      <td>194</td>\n",
       "      <td>Here are the timestamps. Please chec...</td>\n",
       "      <td>here are the timestamps   please che...</td>\n",
       "      <td>timestamps please check sponsors sup...</td>\n",
       "      <td>timestamps please check sponsor supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>2023-11-19 10:59:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Protein research is a major new brea...</td>\n",
       "      <td>protein research is a major new brea...</td>\n",
       "      <td>protein research major new breakthrough</td>\n",
       "      <td>protein research major new breakthrough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Dickinson</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>2023-11-19 10:50:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good one.</td>\n",
       "      <td>this is a good one</td>\n",
       "      <td>good one</td>\n",
       "      <td>good one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john g henderson</td>\n",
       "      <td>2023-11-18 03:47:08+00:00</td>\n",
       "      <td>2023-11-18 03:49:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>A very interesting conversation unti...</td>\n",
       "      <td>a very interesting conversation unti...</td>\n",
       "      <td>interesting conversation end answer ...</td>\n",
       "      <td>interesting conversation end answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arife dickerson</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>2023-11-17 20:49:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>This chick is always in Bilderberg g...</td>\n",
       "      <td>this chick is always in bilderberg g...</td>\n",
       "      <td>chick always bilderberg group meetin...</td>\n",
       "      <td>chick always bilderberg group meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WakeUpnThinkClearly</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>2023-11-17 18:51:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I feel really infinitely ignoran...</td>\n",
       "      <td>now i feel really infinitely ignoran...</td>\n",
       "      <td>feel really infinitely ignorant hope...</td>\n",
       "      <td>feel really infinitely ignorant hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mike Huff</td>\n",
       "      <td>2023-11-17 01:54:59+00:00</td>\n",
       "      <td>2023-11-17 01:56:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>I kept checking the time in hopes th...</td>\n",
       "      <td>i kept checking the time in hopes th...</td>\n",
       "      <td>kept checking time hopes entered sta...</td>\n",
       "      <td>kept checking time hope entered stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author              published_at                updated_at  \\\n",
       "0          Lex Fridman 2022-12-29 17:34:04+00:00 2022-12-29 17:34:04+00:00   \n",
       "1       John Dickinson 2023-11-19 10:59:46+00:00 2023-11-19 10:59:46+00:00   \n",
       "2       John Dickinson 2023-11-19 10:50:38+00:00 2023-11-19 10:50:38+00:00   \n",
       "3     john g henderson 2023-11-18 03:47:08+00:00 2023-11-18 03:49:31+00:00   \n",
       "4      arife dickerson 2023-11-17 20:49:24+00:00 2023-11-17 20:49:24+00:00   \n",
       "5  WakeUpnThinkClearly 2023-11-17 18:51:44+00:00 2023-11-17 18:51:44+00:00   \n",
       "6            Mike Huff 2023-11-17 01:54:59+00:00 2023-11-17 01:56:52+00:00   \n",
       "\n",
       "   likes                                     text  \\\n",
       "0    194  Here are the timestamps. Please chec...   \n",
       "1      0  Protein research is a major new brea...   \n",
       "2      0                      This is a good one.   \n",
       "3      0  A very interesting conversation unti...   \n",
       "4      0  This chick is always in Bilderberg g...   \n",
       "5      0  Now I feel really infinitely ignoran...   \n",
       "6      0  I kept checking the time in hopes th...   \n",
       "\n",
       "                              cleaned_text  \\\n",
       "0  here are the timestamps   please che...   \n",
       "1  protein research is a major new brea...   \n",
       "2                     this is a good one     \n",
       "3  a very interesting conversation unti...   \n",
       "4  this chick is always in bilderberg g...   \n",
       "5  now i feel really infinitely ignoran...   \n",
       "6  i kept checking the time in hopes th...   \n",
       "\n",
       "                             filtered_text  \\\n",
       "0  timestamps please check sponsors sup...   \n",
       "1  protein research major new breakthrough   \n",
       "2                                 good one   \n",
       "3  interesting conversation end answer ...   \n",
       "4  chick always bilderberg group meetin...   \n",
       "5  feel really infinitely ignorant hope...   \n",
       "6  kept checking time hopes entered sta...   \n",
       "\n",
       "                           lemmatized_text  \n",
       "0  timestamps please check sponsor supp...  \n",
       "1  protein research major new breakthrough  \n",
       "2                                 good one  \n",
       "3  interesting conversation end answer ...  \n",
       "4  chick always bilderberg group meetin...  \n",
       "5  feel really infinitely ignorant hope...  \n",
       "6  kept checking time hope entered stat...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"lemmatized_text\"] = corpus.filtered_text.apply(lambda t: lemmatize_text(t))\n",
    "corpus.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cleaned text corpus. Let's save it into `processed` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_pickle(TRANSFORMED_DATA_DIR / \"processed_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
